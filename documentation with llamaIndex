# Import necessary libraries
import nest_asyncio  # Allows nested asyncio loops (useful in Jupyter)
from transformers import AutoTokenizer, AutoModelForCausalLM  # HuggingFace components for models
from llama_index.embeddings.huggingface import HuggingFaceEmbedding  # Embedding model
from llama_index.core import SimpleDirectoryReader, Document  # Document loading and handling
from llama_index.core.schema import MetadataMode  # Controls metadata visibility
from pydantic import BaseModel, Field  # For creating structured data models
from llama_index.core.llms import CustomLLM, CompletionResponse, LLMMetadata  # For custom LLM implementation
from typing import Dict, Any, Iterator, Optional  # Type hints

import torch  # PyTorch for model operations
from llama_index.core.node_parser import SentenceSplitter  # For chunking documents

# Apply nest_asyncio to enable async operations in Jupyter-like environments
nest_asyncio.apply()

# ===== CUSTOM LLM CLASS DEFINITION =====
# This class wraps a local LLaMA model to be used with LlamaIndex
class LocalModelLLM(CustomLLM):
    """
    Custom LLM implementation that uses a local model for inference.
    This adapts a local LLaMA model to work within the LlamaIndex framework.
    """
    # Define the class attributes with Pydantic fields
    model_path: str = Field(description="Path to the local model")
    tokenizer: Any = Field(default=None, exclude=True)  # Exclude from serialization
    model: Any = Field(default=None, exclude=True)  # Exclude from serialization
    
    def __init__(self, model_path: str, **kwargs):
        """
        Initialize the LLM with the model path and load the model and tokenizer.
        
        Args:
            model_path: Path to the local model
            kwargs: Additional arguments to pass to the parent class
        """
        # Initialize with Pydantic
        super().__init__(model_path=model_path, **kwargs)
        
        # Load the tokenizer from the specified path, disabling fast tokenizers
        # which may have compatibility issues with some models
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            model_type="llama",  # Model architecture
            local_files_only=True,  # Only use local files
            torch_dtype=torch.float16,
            device_map="auto",
            use_safetensors=True
            # Alternatively, you can set a specific padding token
            # self.tokenizer.pad_token = "<pad>"
            )
        # Add these lines after loading the tokenizer
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
# Load the model with float16 precision to reduce memory usage
# and distribute across available GPUs
        self.model = AutoModelForCausalLM.from_pretrained(
    self.model_path, 
    torch_dtype=torch.float16, 
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True
)

    
    def complete(self, prompt: str, **kwargs) -> CompletionResponse:
        """
        Generate a completion for the given prompt.
        
        Args:
            prompt: The input text to generate a response for
            kwargs: Additional arguments for generation
            
        Returns:
            CompletionResponse containing the generated text
        """
        # Tokenize the input prompt
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        # Generate text without calculating gradients (inference only)
        with torch.no_grad():
            outputs = self.model.generate(
                inputs["input_ids"].to(self.model.device),  # Move to same device as model
                attention_mask=inputs["attention_mask"].to(self.model.device),  # Add this line
                max_new_tokens=512,  # Limit the number of tokens to generate
                temperature=0.7,  # Controls randomness (higher = more random)
                do_sample=True  # Use sampling instead of greedy decoding
            )
        
        # Decode the generated token IDs back to text
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Remove the prompt from the response if it's included
        if response.startswith(prompt):
            response = response[len(prompt):]
            
        return CompletionResponse(text=response)
    
    def stream_complete(self, prompt: str, **kwargs) -> Iterator[CompletionResponse]:
        """
        Stream the completion for the given prompt.
        This simple implementation just yields the full response at once.
        
        Args:
            prompt: The input text to generate a response for
            kwargs: Additional arguments for generation
            
        Yields:
            CompletionResponse containing the generated text
        """
        # For streaming, we need to implement this method
        # Simple implementation that just yields the full response
        full_response = self.complete(prompt, **kwargs)
        yield full_response
    
    @property
    def metadata(self) -> LLMMetadata:
        """
        Return metadata about the LLM.
        
        Returns:
            LLMMetadata: Metadata about the model
        """
        return LLMMetadata(
            model_name="local_llama3",
            model_path=self.model_path
        )

# ===== MODEL INITIALIZATION =====
# Path to the local LLaMA 3 model
model_path = "D:\Rag\Ollama RAG\Ollama32Rag\Lib\site-packages\llama_models\llama3_2"

# Create the LLM instance with our custom implementation
llm_instance = LocalModelLLM(model_path)

# ===== DOCUMENT LOADING =====
# Load documents from the specified directory
docs = SimpleDirectoryReader(input_dir=r"D:\Rag\Ollama RAG\Ollama32Rag\data").load_data()
print(f"Loaded {len(docs)} documents")

# ===== DOCUMENT FORMATTING DEMONSTRATION =====
# Create an example document to demonstrate metadata formatting
document = Document(
    text="This is a super-customized document",
    metadata={
        "file_name": "super_secret_document.txt",
        "category": "finance",
        "author": "LlamaIndex",
    },
    excluded_llm_metadata_keys=["category"],  # Don't show category to the LLM
    metadata_seperator="\n",  # Separate metadata entries with newlines
    metadata_template="{key}:{value}",  # Format for each metadata entry
    text_template="Metadata:\n{metadata_str}\n-----\nContent:\n{content}",  # Overall document template
)

# Display what the LLM will see (with specified metadata)
print("The LLM sees this: \n", document.get_content(metadata_mode=MetadataMode.LLM))

# ===== CONFIGURE DOCUMENT TEMPLATES =====
# Apply formatting to all loaded documents
for doc in docs:
    # Define the content/metadata template for consistent formatting
    doc.text_template = "Metadata:\n{metadata_str}\n---\nContent:\n{content}"

    # Exclude page label from embedding to improve vector search relevance
    if "page_label" not in doc.excluded_embed_metadata_keys:
        doc.excluded_embed_metadata_keys.append("page_label")

# Print the first document content after modification to confirm changes
print(docs[0].get_content(metadata_mode=MetadataMode.EMBED))

# ===== DOCUMENT PROCESSING COMPONENTS =====
# Import extractors for enhancing document metadata
from llama_index.core.extractors import (
    TitleExtractor,  # Extracts relevant titles from text
    QuestionsAnsweredExtractor,  # Generates potential questions the text answers
)

# Define text splitter for chunking documents into nodes
text_splitter = SentenceSplitter(
    separator=" ",  # Split by spaces between sentences
    chunk_size=1024,  # Target chunk size in characters
    chunk_overlap=128  # Overlap between chunks to maintain context
)

# Create extractors using our custom LLM
title_extractor = TitleExtractor(llm=llm_instance, nodes=5)  # Extract up to 5 candidate titles
qa_extractor = QuestionsAnsweredExtractor(llm=llm_instance, questions=3)  # Generate 3 potential questions

# ===== INGESTION PIPELINE SETUP =====
# Import the ingestion pipeline to coordinate transformations
from llama_index.core.ingestion import IngestionPipeline

# Create the pipeline with our transformations in sequence
pipeline = IngestionPipeline(
    transformations=[
        text_splitter,  # First split into chunks
        title_extractor,  # Then extract titles
        qa_extractor  # Finally extract potential questions
    ]
)

# ===== RUN THE PIPELINE =====
# Process documents through the pipeline
nodes = pipeline.run(
    documents=docs,
    in_place=True,  # Modify the original documents
    show_progress=True,  # Display progress bar
)

# ===== DISPLAY RESULTS =====
# Print a sample of the processed results
print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))

# Wait for user confirmation before exiting
input("Press Enter to exit...")
